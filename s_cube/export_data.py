"""
    Interpolation of the CFD data for the specified fields and time steps onto the coarse grid, sampled with the S^3
    algorithm. Export the interpolated data to HDF5 and XDMF to be able to load them into Paraview
"""
import h5py
import logging
import torch as pt

from time import time
from os.path import join

from flowtorch.analysis import SVD
from sklearn.neighbors import KNeighborsRegressor

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

pt.set_default_dtype(pt.float64)


class Fields:
    """
    class for storing each interpolated field at cell centers and cell nodes.
    """
    def __init__(self, centers: pt.Tensor = None, vertices: pt.Tensor = None):
        self.centers = centers
        self.vertices = vertices


class DataWriter:
    """
    class for interpolating the original CFD data onto the grid generated by the S^3 algorithm, handels the export
    of the data to HDF5 and XDMF.
    """
    def __init__(self, face_ids: pt.Tensor, nodes: pt.Tensor, centers: pt.Tensor, levels: pt.Tensor, save_dir: str,
                 domain_boundaries: list, save_name: str = "data_final_grid", grid_name: str = "final grid",
                 times: list = None) -> None:
        """

        :param face_ids: node indices of the leaf cells of the grid generated by the S^3 algorithm
        :param save_dir: directory to which the XDMF & HDF5 files should be saved to
        :param domain_boundaries: boundaries of the domain (or mask) used when executing the S^3 algorithm, compare to
                                  documentation in 'execute_grid_generation.execute_grid_generation()'
        :param save_name: HDF5 & XDMF file names
        :param grid_name: name of the grid (used inside XDMF)
        :param times: numerical time steps of the simulation as list of int, float or str
        """
        self._save_dir = save_dir
        self._save_name = save_name
        self._grid_name = grid_name
        self._centers = centers
        self._vertices = nodes
        self._levels = levels

        # face_id = node idx making up a face, e.g. [0, 1, 2, 3] make up a face of node no. 0-3
        self._face_id = face_ids
        self._n_vertices = self._vertices.size()[0]
        self._n_faces = self._face_id.size()[0]
        self.n_dimensions = self._centers.size()[1]
        self.times = times
        self.mesh_info = None
        self._boundaries = domain_boundaries
        self._knn = KNeighborsRegressor(n_neighbors=8 if self.n_dimensions == 2 else 26, weights="distance")
        self._interpolated_fields = Fields()
        self._snapshot_counter = 0
        self._field_name = None
        self._initialized = False
        self._finished = False
        self._n_snapshots_total = None
        self._modes = None
        self._mode_coefficients = None
        self._singular_values = None
        self._sqrt_cell_area = None
        self._t_start = time()

    def _write_data_to_hdmf5(self) -> None:
        """
        Write the generated grid and the fields, interpolated at the cell centers and nodes, respectively, to an HDF5
        file for the given number of snapshots.

        :return: None
        """
        # create a writer and datasets for the grid if on initial call
        if not self._initialized:
            logger.info(f"Writing HDF5 file for field {self._field_name}.")
            _writer = h5py.File(join(self._save_dir, f"{self._save_name}_{self._field_name}.h5"), "w")
            _grid_data = _writer.create_group("grid")
            _grid_data.create_dataset("faces", data=self._face_id)
            _grid_data.create_dataset("vertices", data=self._vertices)
            _grid_data.create_dataset("centers", data=self._centers)
            _vertices = _writer.create_group(f"{self._field_name}_vertices")
            _center = _writer.create_group(f"{self._field_name}_center")

            # add field for the cell levels
            _writer.create_dataset("levels", data=self._levels)
            self._initialized = True

        else:
            # once the grid is written, we can append the field data
            _writer = h5py.File(join(self._save_dir, f"{self._save_name}_{self._field_name}.h5"), "a")
            _vertices = _writer[f"{self._field_name}_vertices"]
            _center = _writer[f"{self._field_name}_center"]

        # write the datasets for each given time step, we already updated the snapshot counter after fitting the
        # field, so we need to subtract it to get the starting dt
        t_start = self._snapshot_counter - self._interpolated_fields.centers.size(-1)
        t_end = self._snapshot_counter

        # create a group for each specified field
        for i, t in enumerate(self.times[t_start:t_end]):
            # in case we have a scalar, we need to remove the additional dimension we created for fitting the data
            if self._interpolated_fields.centers.size(1) == 1:
                _center.create_dataset(str(t), data=self._interpolated_fields.centers.squeeze(1)[:, i])
                _vertices.create_dataset(str(t), data=self._interpolated_fields.vertices.squeeze(1)[:, i])
            # in case we have a vector
            else:
                _center.create_dataset(str(t), data=self._interpolated_fields.centers[:, :, i])
                _vertices.create_dataset(str(t), data=self._interpolated_fields.vertices[:, :, i])

        # close hdf file
        _writer.close()

        # check if we have written all snapshots, if yes, then write the XDMF file
        if self._snapshot_counter == self._n_snapshots_total:
            self._write_xdmf()

            # reset properties for the next field, we don't need to reset the computed cell areas since the grid remains
            # the same for all fields
            self._interpolated_fields = Fields()
            self._snapshot_counter = 0
            self._initialized = False
            logger.info(f"Finished export of field {self._field_name} in {round(time() - self._t_start, 3)}s.")
            self._t_start = time()

    def _fit_data(self, _coord: pt.Tensor, _data: pt.Tensor, _field_name: str, _n_snapshots_total: int = None) -> None:
        """
        Interpolate the CFD data executed on the original grid onto the newly, coarser grid generated by the S^3
        algorithm. The original field is interpolated at the cell centers as well as at the cell nodes.

        Note: the variables 'centers' & 'vertices' used in this method are denoting the values of the field at center
        and nodes of each cell (not the node coordinates of the generated mesh)

        :param _coord: the coordinates of the original grid used in CFD
        :param _data: the original field data
        :param _field_name: name of the field, which should be exported, e.g. 'p' for pressure field
        :param _n_snapshots_total: number of snapshots in total, which should be exported. If 'None', it is assumed
                                   that the provided data are all available snapshots
        :return: None
        """
        # check if the field has the correct shape -> scalar fields need to be unsqueezed at dim=1 as:
        # [N_cells, N_dimensions, N_snapshots] (vector field) or [N_cells, 1, N_snapshots] (scalar field)
        assert len(_data.size()) == 3, "The provided field must have the shape '[N_cells, N_dimensions, N_snapshots]'" \
                                       "for a vector field and '[N_cells, 1, N_snapshots]' for a scalar field"

        # save the provided field name for writing the HDF5 & XDMF files
        self._field_name = _field_name

        if not self._initialized:
            logger.info(f"Starting interpolation and export of field {self._field_name}.")

        # determine the required size of the data matrix
        self._n_snapshots_total = _n_snapshots_total if _n_snapshots_total is not None else _data.size()[-1]

        # create empty tensors for the field values at centers & vertices with dimensions
        # [N_cells, N_dimensions, N_snapshots_currently] each call to allow variable batch sizes
        self._interpolated_fields.centers = pt.zeros((self._centers.size()[0], _data.size()[1], _data.size()[2]),
                                                     dtype=pt.float32)
        self._interpolated_fields.vertices = pt.zeros((self._vertices.size()[0], _data.size()[1], _data.size()[2]),
                                                      dtype=pt.float32)

        # fit the KNN and interpolate the data, we need to predict each dimension separately (otherwise dim. mismatch)
        for dimension in range(_data.size()[1]):
            self._knn.fit(_coord, _data[:, dimension, :])
            self._interpolated_fields.vertices[:, dimension, :] = pt.from_numpy(self._knn.predict(self._vertices))
            self._interpolated_fields.centers[:, dimension, :] = pt.from_numpy(self._knn.predict(self._centers))

        # update the number of snapshots we already interpolated
        self._snapshot_counter += _data.size()[-1]

    def _write_xdmf(self) -> None:
        """
        Write the XDMF file corresponding to the HDF5 file for the interpolated field.
        Can be used to import and visualize the data in Paraview.

        :return: None
        """
        logger.info(f"Writing XDMF file for field {self._field_name}.")

        _file_name = f"{self._save_name}_{self._field_name}"
        _global_header = f'<?xml version="1.0"?>\n<!DOCTYPE Xdmf SYSTEM "Xdmf.dtd" []>\n<Xdmf Version="2.0">\n' \
                         f'<Domain>\n<Grid Name="{self._grid_name}" GridType="Collection" CollectionType="temporal">\n'
        _grid_type = "Quadrilateral" if self.n_dimensions == 2 else "Hexahedron"
        _dims = "XY" if self.n_dimensions == 2 else "XYZ"

        # write the corresponding XDMF file
        with open(join(self._save_dir, f"{_file_name}.xdmf"), "w") as f_out:
            # write global header
            f_out.write(_global_header)

            # loop over all available time steps and write all specified data to XDMF & HDF5 files, since we have the
            # HDMF5 file written completely for al specified snapshots, we can now iterate over all time steps
            for i, t in enumerate(self.times):
                # write grid specific header
                tmp = f'<Grid Name="{self._grid_name} {t}" GridType="Uniform">\n<Time Value="{t}"/>\n' \
                      f'<Topology TopologyType="{_grid_type}" NumberOfElements="{self._n_faces}">\n' \
                      f'<DataItem Format="HDF" DataType="Int" Dimensions="{self._n_faces} ' \
                      f'{pow(2, self.n_dimensions)}">\n'
                f_out.write(tmp)

                # include the grid data from the HDF5 file
                f_out.write(f"{_file_name}.h5:/grid/faces\n")

                # write geometry part
                f_out.write(f'</DataItem>\n</Topology>\n<Geometry GeometryType="{_dims}">\n'
                            f'<DataItem Rank="2" Dimensions="{self._n_vertices} {self.n_dimensions}" '
                            f'NumberType="Float" Format="HDF">\n')

                # write coordinates of vertices
                f_out.write(f"{_file_name}.h5:/grid/vertices\n</DataItem>\n</Geometry>\n")

                # write the interpolated fields
                # determine 2nd dimension (scalar vs. vector)
                if len(self._interpolated_fields.centers.size()) == 2:
                    _second_dim = 1
                else:
                    _second_dim = self._interpolated_fields.centers.size()[1]

                # write the levels into the first time step
                if i == 0:
                    f_out.write(f'<Attribute Name="levels" Center="Cell">\n<DataItem Format="HDF" '
                                f'Dimensions="{self._levels.size()[0]} {1}">\n')
                    f_out.write(f"{_file_name}.h5:/levels\n</DataItem>\n</Attribute>\n")

                # write header for field
                f_out.write(f'<Attribute Name="{self._field_name}" Center="Cell">\n<DataItem Format="HDF" '
                            f'Dimensions="{self._interpolated_fields.centers.size()[0]} {_second_dim}">\n')

                # write interpolated field at the cell center
                f_out.write(f"{_file_name}.h5:/{self._field_name}_center/{t}\n</DataItem>\n</Attribute>\n")

                # then do the same for field at the vertices
                f_out.write(f'<Attribute Name="{self._field_name}" Center="Node">\n<DataItem Format="HDF" '
                            f'Dimensions="{self._interpolated_fields.vertices.size()[0]} {_second_dim}">\n')
                f_out.write(f"{_file_name}.h5:/{self._field_name}_vertices/{t}\n</DataItem>\n</Attribute>\n")

                # write end tag of the current grid
                f_out.write('</Grid>\n')

            # write rest of file
            f_out.write('</Grid>\n</Domain>\n</Xdmf>')

    def export_data(self, _coord: pt.Tensor, _data: pt.Tensor, _field_name: str,
                    _n_snapshots_total: int = None) -> None:
        """
        Interpolate the provided (original) CFD data onto the generated grid by S^3 and write it to a HDF5 and XDMF
        file for all given time steps.

        :param _coord: the coordinates of the original grid used in CFD
        :param _data: the original field data with dimension [N_cells, N_dimensions, N_snapshots].
                      N_snapshots can either be all snapshots, a batch of snapshots or just a single snapshot
        :param _field_name: name of the field, which should be exported, e.g. 'p' for pressure field
        :param _n_snapshots_total: number of snapshots in total, which should be exported. If 'None', it is assumed
                                   that the provided data are all available snapshots
        :return: None
        """
        self._fit_data(_coord, _data, _field_name, _n_snapshots_total)
        self._write_data_to_hdmf5()

    def compute_svd(self, field_name: str) -> None:
        """
        Computes an SVd of the interpolated field.
        It is assumed that the field was already interpolated and exported completely (all snapshots) to HDF5 file.
        It is further assumed that the current grid set in the DataWriter class is the same as used to export the field.
        Although the SVD is performed on the complete dataset, only the modes until the optimal rank are saved.
        The singular values and left singular vectors are saved in full.
        For more information on the determination of the optimal rank, it is referred to the flowtorch documentation:

        https://flowmodelingcontrol.github.io/flowtorch-docs/1.2/flowtorch.analysis.html#flowtorch.analysis.svd.SVD.opt_rank

        :param field_name: name of the field for which the SVD should be computed
        :return: None
        """
        logger.info(f"Computing SVD for field {field_name}.")

        # load the reduced dataset from HDF5 file, create datamatrix
        _field = self._construct_data_matrix_from_hdf5(field_name)

        # compute the sqrt of the cell area (2D) / volume (3D)
        if self._sqrt_cell_area is None:
            self._sqrt_cell_area = (1/pow(2, self.n_dimensions) * pow(self.mesh_info["size_initial_cell"] /
                                                                      pow(2, self._levels), self.n_dimensions)).sqrt()

        # subtract the temporal mean
        _field -= pt.mean(_field, dim=-1).unsqueeze(-1)

        if len(_field.size()) == 2:
            # multiply by the sqrt of the cell areas to weight their contribution
            _field *= self._sqrt_cell_area
            svd = SVD(_field, rank=_field.size(-1))
            self._modes = svd.U[:, :svd.opt_rank] / self._sqrt_cell_area
        else:
            _field *= self._sqrt_cell_area.unsqueeze(-1)

            # stack the data of all components for the SVD
            orig_shape = _field.size()
            _field = _field.reshape((orig_shape[1] * orig_shape[0], orig_shape[-1]))
            svd = SVD(_field, rank=_field.size(-1))

            # reshape the data back to ux, uy, uz
            new_shape = (orig_shape[0], orig_shape[1], svd.rank)
            self._modes = svd.U.reshape(new_shape)[:, :, :svd.opt_rank] / self._sqrt_cell_area.unsqueeze(-1)

        # write HDF5 file
        self._mode_coefficients = svd.V
        self._singular_values = svd.s
        self._write_hfd5_for_svd(field_name)

        # write XDMF file
        self._write_xdmf_for_svd(field_name)

    def _construct_data_matrix_from_hdf5(self, _field_name: str) -> pt.Tensor:
        """
        Reconstruct the full datamatrix of the interpolated field from the temporal grid structure inside the HDF5 file.

        :param _field_name: name of the field for which the SVD should be computed
        :return: tensor with the dimensions [N_cells, N_dimensions, N_snapshots] containing all snapshots of the
                 interpolated field
        """
        try:
            hdf_file = h5py.File(join(self._save_dir, f"{self._save_name}_{_field_name}.h5"), "r")
        except FileNotFoundError:
            logger.error(f"HDF5 file with the interpolated field {_field_name} not found. Make sure the specified field"
                         f" exists and was already interpolated onto the generated grid.")
            exit(0)

        # assemble the data matrix
        keys = list(hdf_file[f"{_field_name}_center"].keys())
        shape = hdf_file[f"{_field_name}_center"][keys[0]].shape
        if len(shape) == 1:
            data_out = pt.zeros((shape[0], len(keys)), dtype=pt.float32)
        else:
            data_out = pt.zeros((shape[0], shape[1], len(keys)), dtype=pt.float32)
        for i, k in enumerate(keys):
            if len(shape) == 1:
                data_out[:, i] = pt.from_numpy(hdf_file.get(f"{_field_name}_center/{k}")[()])
            else:
                data_out[:, :, i] = pt.from_numpy(hdf_file.get(f"{_field_name}_center/{k}")[()])

        return data_out

    def _write_hfd5_for_svd(self, _field_name: str) -> None:
        """
        Write the HDF5 file storing the results from the SVD.

        :param _field_name: name of the field for which the SVD should be computed
        :return: None
        """
        _writer = h5py.File(join(self._save_dir, f"{self._save_name}_svd_{_field_name}.h5"), "w")
        _grid_data = _writer.create_group("grid")
        _grid_data.create_dataset("faces", data=self._face_id)
        _grid_data.create_dataset("vertices", data=self._vertices)
        _grid_data.create_dataset("centers", data=self._centers)

        # if all snapshots of the interpolated fields are available, perform an SVD for each component of the
        # field (will be extended to arbitrary batch sizes once this is working)
        if len(self._modes.size()) == 2:
            _writer.create_dataset("mode", data=self._modes)
        else:
            dims = ["x", "y", "z"]
            for i in range(self._modes.size(1)):
                _writer.create_dataset(f"mode_{dims[i]}", data=self._modes[:, i, :].squeeze())

        # write the mode coefficients and singular values only to the HDF5 file (not XDMF)
        _writer.create_dataset("mode_coefficients", data=self._mode_coefficients)
        _writer.create_dataset("singular_values", data=self._singular_values)
        _writer.create_dataset("sqrt_cell_area", data=self._sqrt_cell_area)

        # close hdf file
        _writer.close()

    def _write_xdmf_for_svd(self, _field_name: str) -> None:
        """
        Write the XDMF file referencing the modes resulting from the SVD in the corresponding HDF5 file.

        :param _field_name: name of the field for which the SVD should be computed
        :return: None
        """
        _grid_type = "Quadrilateral" if self.n_dimensions == 2 else "Hexahedron"
        _file_name = f"{self._save_name}_svd_{_field_name}"
        _dims = "XY" if self.n_dimensions == 2 else "XYZ"

        _global_header = f'<?xml version="1.0"?>\n<!DOCTYPE Xdmf SYSTEM "Xdmf.dtd" []>\n<Xdmf Version="2.0">\n' \
                         f'<Domain>\n<Grid Name="{self._grid_name}" GridType="Uniform">\n' \
                         f'<Topology TopologyType="{_grid_type}" NumberOfElements="{self._n_faces}">\n'\
                         f'<DataItem Format="HDF" DataType="Int" Dimensions="{self._n_faces} '\
                         f'{pow(2, self.n_dimensions)}">\n'

        # write the corresponding XDMF file
        with open(join(self._save_dir, f"{_file_name}.xdmf"), "w") as f_out:
            # write global header
            f_out.write(_global_header)

            # include the grid data from the HDF5 file
            f_out.write(f"{_file_name}.h5:/grid/faces\n")

            # write geometry part
            f_out.write(f'</DataItem>\n</Topology>\n<Geometry GeometryType="{_dims}">\n'
                        f'<DataItem Rank="2" Dimensions="{self._n_vertices} {self.n_dimensions}" '
                        f'NumberType="Float" Format="HDF">\n')

            # write coordinates of vertices
            f_out.write(f"{_file_name}.h5:/grid/vertices\n")

            # write end tags
            f_out.write("</DataItem>\n</Geometry>\n")

            # write POD modes to the last time step
            if len(self._modes.size()) == 2:
                f_out.write(f'<Attribute Name="mode" Center="Cell">\n<DataItem Format="HDF" Dimensions='
                            f'"{self._modes.size(0)} {self._modes.size(-1)}">\n')
                f_out.write(f"{_file_name}.h5:/mode\n</DataItem>\n</Attribute>\n")
            else:
                for d in ["x", "y", "z"]:
                    f_out.write(f'<Attribute Name="mode_{d}" Center="Cell">\n<DataItem Format="HDF" Dimensions='
                                f'"{self._modes.size(0)} {self._modes.size(-1)}">\n')
                    f_out.write(f"{_file_name}.h5:/mode_{d}\n</DataItem>\n</Attribute>\n")

            # write the sqrt cell area
            f_out.write(f'<Attribute Name="sqrt_cell_area" Center="Cell">\n<DataItem Format="HDF" Dimensions='
                        f'"{self._sqrt_cell_area.size(0)} {self._sqrt_cell_area.size(-1)}">\n')
            f_out.write(f"{_file_name}.h5:/sqrt_cell_area\n</DataItem>\n</Attribute>\n</Grid>\n</Domain>\n</Xdmf>")


if __name__ == "__main__":
    pass
