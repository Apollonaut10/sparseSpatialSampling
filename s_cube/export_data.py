"""
    Interpolation of the CFD data for the specified fields and time steps onto the coarse grid, sampled with the S^3
    algorithm. Export the interpolated data to HDF5 and XDMF to be able to load them into Paraview.
    Further, some helper functions to make the export of OpenFoam field more convenient
"""
import h5py
import logging
import torch as pt

from time import time
from os.path import join
from typing import Union

from flowtorch.data import FOAMDataloader, mask_box
from sklearn.neighbors import KNeighborsRegressor

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

pt.set_default_dtype(pt.float64)


class Fields:
    """
    class for storing each interpolated field at cell centers and cell nodes.
    """
    def __init__(self, centers: pt.Tensor = None, vertices: pt.Tensor = None):
        self.centers = centers
        self.vertices = vertices


class DataWriter:
    """
    class for interpolating the original CFD data onto the grid generated by the S^3 algorithm, handels the export
    of the data to HDF5 and XDMF.
    """
    def __init__(self, face_ids: pt.Tensor, nodes: pt.Tensor, centers: pt.Tensor, levels: pt.Tensor, save_dir: str,
                 domain_boundaries: list, save_name: str = "data_final_grid", grid_name: str = "final grid",
                 times: list = None) -> None:
        """

        :param face_ids: node indices of the leaf cells of the grid generated by the S^3 algorithm
        :param save_dir: directory to which the XDMF & HDF5 files should be saved to
        :param domain_boundaries: boundaries of the domain (or mask) used when executing the S^3 algorithm, compare to
                                  documentation in 'execute_grid_generation.execute_grid_generation()'
        :param save_name: HDF5 & XDMF file names
        :param grid_name: name of the grid (used inside XDMF)
        :param times: numerical time steps of the simulation as list of int, float or str
        """
        self._save_dir = save_dir
        self._save_name = save_name
        self._grid_name = grid_name
        self._centers = centers
        self._vertices = nodes
        self._levels = levels
        self._metric = None

        # face_id = node idx making up a face, e.g. [0, 1, 2, 3] make up a face of node no. 0-3
        self._face_id = face_ids
        self._n_vertices = self._vertices.size()[0]
        self._n_faces = self._face_id.size()[0]
        self.n_dimensions = self._centers.size()[1]
        self.times = times
        self.mesh_info = None
        self._boundaries = domain_boundaries
        self._knn = KNeighborsRegressor(n_neighbors=8 if self.n_dimensions == 2 else 26, weights="distance")
        self._interpolated_fields = Fields()
        self._snapshot_counter = 0
        self._field_name = None
        self._initialized = False
        self._finished = False
        self._n_snapshots_total = None
        self._t_start = time()

    def _write_data_to_hdmf5(self) -> None:
        """
        Write the generated grid and the fields, interpolated at the cell centers and nodes, respectively, to an HDF5
        file for the given number of snapshots.

        :return: None
        """
        # create a writer and datasets for the grid if on initial call
        if not self._initialized:
            logger.info(f"Writing HDF5 file for field {self._field_name}.")
            _writer = h5py.File(join(self._save_dir, f"{self._save_name}_{self._field_name}.h5"), "w")
            _grid_data = _writer.create_group("grid")
            _grid_data.create_dataset("faces", data=self._face_id)
            _grid_data.create_dataset("vertices", data=self._vertices)
            _grid_data.create_dataset("centers", data=self._centers)
            _vertices = _writer.create_group(f"{self._field_name}_vertices")
            _center = _writer.create_group(f"{self._field_name}_center")

            # add field for the cell levels
            _writer.create_dataset("levels", data=self._levels)
            _writer.create_dataset("metric", data=self._metric)
            _writer.create_dataset("size_initial_cell", data=self.mesh_info["size_initial_cell"])
            self._initialized = True

        else:
            # once the grid is written, we can append the field data
            _writer = h5py.File(join(self._save_dir, f"{self._save_name}_{self._field_name}.h5"), "a")
            _vertices = _writer[f"{self._field_name}_vertices"]
            _center = _writer[f"{self._field_name}_center"]

        # write the datasets for each given time step, we already updated the snapshot counter after fitting the
        # field, so we need to subtract it to get the starting dt
        t_start = self._snapshot_counter - self._interpolated_fields.centers.size(-1)
        t_end = self._snapshot_counter

        # create a group for each specified field
        for i, t in enumerate(self.times[t_start:t_end]):
            # in case we have a scalar, we need to remove the additional dimension we created for fitting the data
            if self._interpolated_fields.centers.size(1) == 1:
                _center.create_dataset(str(t), data=self._interpolated_fields.centers.squeeze(1)[:, i])
                _vertices.create_dataset(str(t), data=self._interpolated_fields.vertices.squeeze(1)[:, i])
            # in case we have a vector
            else:
                _center.create_dataset(str(t), data=self._interpolated_fields.centers[:, :, i])
                _vertices.create_dataset(str(t), data=self._interpolated_fields.vertices[:, :, i])

        # close hdf file
        _writer.close()

        # check if we have written all snapshots, if yes, then write the XDMF file
        if self._snapshot_counter == self._n_snapshots_total:
            self._write_xdmf()

            # reset properties for the next field, we don't need to reset the computed cell areas since the grid remains
            # the same for all fields
            self._interpolated_fields = Fields()
            self._snapshot_counter = 0
            self._initialized = False
            logger.info(f"Finished export of field {self._field_name} in {round(time() - self._t_start, 3)}s.")
            self._t_start = time()

    def _fit_data(self, _coord: pt.Tensor, _data: pt.Tensor, _field_name: str, _n_snapshots_total: int = None) -> None:
        """
        Interpolate the CFD data executed on the original grid onto the newly, coarser grid generated by the S^3
        algorithm. The original field is interpolated at the cell centers as well as at the cell nodes.

        Note: the variables 'centers' & 'vertices' used in this method are denoting the values of the field at center
        and nodes of each cell (not the node coordinates of the generated mesh)

        :param _coord: the coordinates of the original grid used in CFD
        :param _data: the original field data
        :param _field_name: name of the field, which should be exported, e.g. 'p' for pressure field
        :param _n_snapshots_total: number of snapshots in total, which should be exported. If 'None', it is assumed
                                   that the provided data are all available snapshots
        :return: None
        """
        # check if the field has the correct shape -> scalar fields need to be unsqueezed at dim=1 as:
        # [N_cells, N_dimensions, N_snapshots] (vector field) or [N_cells, 1, N_snapshots] (scalar field)
        assert len(_data.size()) == 3, "The provided field must have the shape '[N_cells, N_dimensions, N_snapshots]'" \
                                       "for a vector field and '[N_cells, 1, N_snapshots]' for a scalar field"

        # save the provided field name for writing the HDF5 & XDMF files
        self._field_name = _field_name

        if not self._initialized:
            logger.info(f"Starting interpolation and export of field {self._field_name}.")

        # fit the metric at some point. If the number of coordinates is still the same as the CFD grid, then we haven't
        # fitted it yet.
        if self._metric.size(0) == _coord.size(0):
            self._knn.fit(_coord, self._metric)

            # overwrite the metric with the interpolated one
            self._metric = pt.from_numpy(self._knn.predict(self._centers))

        # determine the required size of the data matrix
        self._n_snapshots_total = _n_snapshots_total if _n_snapshots_total is not None else _data.size()[-1]

        # create empty tensors for the field values at centers & vertices with dimensions
        # [N_cells, N_dimensions, N_snapshots_currently] each call to allow variable batch sizes
        self._interpolated_fields.centers = pt.zeros((self._centers.size()[0], _data.size()[1], _data.size()[2]),
                                                     dtype=pt.float32)
        self._interpolated_fields.vertices = pt.zeros((self._vertices.size()[0], _data.size()[1], _data.size()[2]),
                                                      dtype=pt.float32)

        # fit the KNN and interpolate the data, we need to predict each dimension separately (otherwise dim. mismatch)
        for dimension in range(_data.size()[1]):
            self._knn.fit(_coord, _data[:, dimension, :])
            self._interpolated_fields.vertices[:, dimension, :] = pt.from_numpy(self._knn.predict(self._vertices))
            self._interpolated_fields.centers[:, dimension, :] = pt.from_numpy(self._knn.predict(self._centers))

        # update the number of snapshots we already interpolated
        self._snapshot_counter += _data.size()[-1]

    def _write_xdmf(self) -> None:
        """
        Write the XDMF file corresponding to the HDF5 file for the interpolated field.
        Can be used to import and visualize the data in Paraview.

        :return: None
        """
        logger.info(f"Writing XDMF file for field {self._field_name}.")

        _file_name = f"{self._save_name}_{self._field_name}"
        _global_header = f'<?xml version="1.0"?>\n<!DOCTYPE Xdmf SYSTEM "Xdmf.dtd" []>\n<Xdmf Version="2.0">\n' \
                         f'<Domain>\n<Grid Name="{self._grid_name}" GridType="Collection" CollectionType="temporal">\n'
        _grid_type = "Quadrilateral" if self.n_dimensions == 2 else "Hexahedron"
        _dims = "XY" if self.n_dimensions == 2 else "XYZ"

        # write the corresponding XDMF file
        with open(join(self._save_dir, f"{_file_name}.xdmf"), "w") as f_out:
            # write global header
            f_out.write(_global_header)

            # loop over all available time steps and write all specified data to XDMF & HDF5 files, since we have the
            # HDMF5 file written completely for al specified snapshots, we can now iterate over all time steps
            for i, t in enumerate(self.times):
                # write grid specific header
                tmp = f'<Grid Name="{self._grid_name} {t}" GridType="Uniform">\n<Time Value="{t}"/>\n' \
                      f'<Topology TopologyType="{_grid_type}" NumberOfElements="{self._n_faces}">\n' \
                      f'<DataItem Format="HDF" DataType="Int" Dimensions="{self._n_faces} ' \
                      f'{pow(2, self.n_dimensions)}">\n'
                f_out.write(tmp)

                # include the grid data from the HDF5 file
                f_out.write(f"{_file_name}.h5:/grid/faces\n")

                # write geometry part
                f_out.write(f'</DataItem>\n</Topology>\n<Geometry GeometryType="{_dims}">\n'
                            f'<DataItem Rank="2" Dimensions="{self._n_vertices} {self.n_dimensions}" '
                            f'NumberType="Float" Precision="8" Format="HDF">\n')

                # write coordinates of vertices
                f_out.write(f"{_file_name}.h5:/grid/vertices\n</DataItem>\n</Geometry>\n")

                # write the interpolated fields
                # determine 2nd dimension (scalar vs. vector)
                if len(self._interpolated_fields.centers.size()) == 2:
                    _second_dim = 1
                else:
                    _second_dim = self._interpolated_fields.centers.size()[1]

                # write the levels and metric into the first time step
                if i == 0:
                    f_out.write(f'<Attribute Name="levels" AttributeType="Vector" Center="Cell">\n<DataItem '
                                f'NumberType="Float" Precision="8" Format="HDF" '
                                f'Dimensions="{self._levels.size()[0]} {1}">\n')
                    f_out.write(f"{_file_name}.h5:/levels\n</DataItem>\n</Attribute>\n")
                    f_out.write(f'<Attribute Name="metric" AttributeType="Vector" Center="Cell">\n<DataItem '
                                f'NumberType="Float" Precision="8" Format="HDF" '
                                f'Dimensions="{self._metric.size()[0]} {1}">\n')
                    f_out.write(f"{_file_name}.h5:/metric\n</DataItem>\n</Attribute>\n")

                # write header for field
                f_out.write(f'<Attribute Name="{self._field_name}" AttributeType="Vector" Center="Cell">\n<DataItem '
                            f'NumberType="Float" Precision="8" Format="HDF" '
                            f'Dimensions="{self._interpolated_fields.centers.size()[0]} {_second_dim}">\n')

                # write interpolated field at the cell center
                f_out.write(f"{_file_name}.h5:/{self._field_name}_center/{t}\n</DataItem>\n</Attribute>\n")

                # then do the same for field at the vertices
                f_out.write(f'<Attribute Name="{self._field_name}" AttributeType="Vector" Center="Node">\n<DataItem '
                            f'NumberType="Float" Precision="8" Format="HDF" '
                            f'Dimensions="{self._interpolated_fields.vertices.size()[0]} {_second_dim}">\n')
                f_out.write(f"{_file_name}.h5:/{self._field_name}_vertices/{t}\n</DataItem>\n</Attribute>\n")

                # write end tag of the current grid
                f_out.write('</Grid>\n')

            # write rest of file
            f_out.write('</Grid>\n</Domain>\n</Xdmf>')

    def export_data(self, _coord: pt.Tensor, _data: pt.Tensor, _field_name: str,
                    _n_snapshots_total: int = None) -> None:
        """
        Interpolate the provided (original) CFD data onto the generated grid by S^3 and write it to a HDF5 and XDMF
        file for all given time steps.

        :param _coord: the coordinates of the original grid used in CFD
        :param _data: the original field data with dimension [N_cells, N_dimensions, N_snapshots].
                      N_snapshots can either be all snapshots, a batch of snapshots or just a single snapshot
        :param _field_name: name of the field, which should be exported, e.g. 'p' for pressure field
        :param _n_snapshots_total: number of snapshots in total, which should be exported. If 'None', it is assumed
                                   that the provided data are all available snapshots
        :return: None
        """
        self._fit_data(_coord, _data, _field_name, _n_snapshots_total)
        self._write_data_to_hdmf5()


def load_original_Foam_fields(_load_dir: str, _n_dimensions: int, _boundaries: list,
                              _field_names: Union[list, str] = None, _write_times: Union[list, str] = None,
                              _get_field_names_and_times: bool = False):
    """
    function for loading fields from OpenFoam either for a single field, multiple fields at once, single time steps
    or multiple time steps at once.

    :param _load_dir: path to the original CFD data
    :param _n_dimensions: number of physical dimensions
    :param _boundaries: boundaries of the numerical domain, need to be the same as used for the execution of the S^3
    :param _field_names: names of the fields which should be exported
    :param _write_times: numerical time steps which should be exported, can be either a str or a list of str
    :param _get_field_names_and_times: returns available field names at first available time steps and write times
    :return: if _get_field_names_and_times = True: available field names at first available time steps and write times
             if False: the specified field, if field can't be found, 'None' is returned
    """
    # create foam loader object
    loader = FOAMDataloader(_load_dir)

    # check which fields and write times are available
    if _get_field_names_and_times:
        _write_times = [t for t in loader.write_times[1:]]
        return _write_times, loader.field_names[_write_times[0]]

    else:
        # load vertices
        vertices = loader.vertices if _n_dimensions == 3 else loader.vertices[:, :2]
        mask = mask_box(vertices, lower=_boundaries[0], upper=_boundaries[1])

        # the coordinates are independent of the field
        # stack the coordinates to tuples
        coord = pt.stack([pt.masked_select(vertices[:, d], mask) for d in range(_n_dimensions)], dim=1)

        # get all available time steps, skip the zero folder
        if _write_times is None:
            _write_times = [t for t in loader.write_times[1:]]
        elif type(_write_times) is str:
            _write_times = [_write_times]

        # in case there are no fields specified, take all available fields
        if _field_names is None:
            _field_names = loader.field_names[_write_times[0]]
        elif type(_field_names) is str:
            _field_names = [_field_names]

        # assemble data matrix for each field and interpolate the values onto the coarser grid
        _fields_out = []
        for field in _field_names:
            # determine if we have a vector or a scalar
            try:
                _field_size = loader.load_snapshot(field, _write_times[0]).size()
            except ValueError:
                logger.warning(f"\tField '{field}' is not available. Skipping this field...")
                continue

            if len(_field_size) == 1:
                data = pt.zeros((mask.sum().item(), len(_write_times)), dtype=pt.float32)
            else:
                data = pt.zeros((mask.sum().item(), _field_size[1], len(_write_times)), dtype=pt.float32)
                mask = mask.unsqueeze(-1).expand(_field_size)

            try:
                for i, t in enumerate(_write_times):
                    # load the field
                    if len(_field_size) == 1:
                        data[:, i] = pt.masked_select(loader.load_snapshot(field, t), mask)
                    else:
                        # we always need to export all dimensions of a vector, because for 2D we don't know in which
                        # plane the flow problem is defined
                        data[:, :, i] = pt.masked_select(loader.load_snapshot(field, t), mask).reshape([coord.size(0),
                                                                                                        _field_size[1]])

            # if fields are written out only for specific parts of the domain, this leads to dimension mismatch between
            # the field and the mask. The mask takes all cells in the specified area, but the field is only written out
            # in a part of this mask.
            except RuntimeError:
                logger.warning(f"\tField '{field}' is does not match the size of the masked domain. Skipping this "
                               f"field...")
                continue

            # since the size of data matrix must be: [N_cells, N_dimensions, N_snapshots] (vector field) or
            # [N_cells, 1, N_snapshots] (scalar field); unsqueeze if we have a scalar field
            if len(_field_size) == 1:
                data = data.unsqueeze(1)

            _fields_out.append([coord, data])

        if len(_fields_out) > 1:
            return _fields_out
        elif not _fields_out:
            return None, None
        else:
            return _fields_out[0]


def export_openfoam_fields(datawriter: DataWriter, load_path: str, boundaries: list,
                           batch_size: int = None, fields: Union[list, str] = None) -> None:
    """
    Wrapper function for interpolating the original CFD data executed with OpenFoam onto the generated grid with the
    S^3 algorithm. If the data was not generated with OpenFoam, the 'export_data()' method of the DataWriter class needs
    to be called directly with the CFD data and coordinates of the original grid as, e.g., implemented in
    'examples/s3_for_OAT15_airfoil.py'

    Important Note: this wrapper function only exports fields that are available in all time steps based on the
                    available fields in the first time step. If fields that are only present at every Nth time step
                    should be exported, this function cannot be used. Instead, the time steps and field name have to
                    be given directly to the export_data method as it is done in 'examples/s3_for_OAT15_airfoil.py'

                    E.g.:
                            times = [0.1, 0.2, 0.3] \n
                            export = execute_grid_generation(...) \n
                            export.times = times    # times needs to be a list of str\n
                            export.export_data(...) \n

    :param datawriter: Datawriter object resulting from the refinement with S^3
    :param load_path: path to the original CFD data
    :param boundaries: boundaries used for generating the mesh
    :param batch_size: batch size, number of snapshots which should be interpolated and exported at once. If 'None',
                       then all available snapshots will be exported at once
    :param fields: fields to export, either str or list[str]. If 'None' then all available fields at the first time
                   step will be exported
    :return: None
    """
    # get the available time steps and field names based on the fields available in the first time step
    if fields is None:
        _, fields = load_original_Foam_fields(load_path, datawriter.n_dimensions, boundaries,
                                              _get_field_names_and_times=True)

    # save time steps of all snapshots if not already provided when starting the refinement, needs to be list[str]
    if datawriter.times is None:
        times, _ = load_original_Foam_fields(load_path, datawriter.n_dimensions, boundaries,
                                             _get_field_names_and_times=True)
        datawriter.times = times

    batch_size = batch_size if batch_size is not None else len(datawriter.times)

    if type(fields) is str:
        fields = [fields]

    # interpolate and export the specified fields
    for f in fields:
        counter = 1
        if not len(datawriter.times) % batch_size:
            n_batches = int(len(datawriter.times) / batch_size)
        else:
            n_batches = int(len(datawriter.times) / batch_size) + 1

        for t in pt.arange(0, len(datawriter.times), step=batch_size).tolist():
            logger.info(f"Exporting batch {counter} / {n_batches}")
            coordinates, data = load_original_Foam_fields(load_path, datawriter.n_dimensions, boundaries,
                                                          _field_names=f,
                                                          _write_times=datawriter.times[t:t + batch_size])

            # in case the field is not available, the export()-method will return None
            if data is not None:
                datawriter.export_data(coordinates, data, f, _n_snapshots_total=len(datawriter.times))
            counter += 1


if __name__ == "__main__":
    pass
