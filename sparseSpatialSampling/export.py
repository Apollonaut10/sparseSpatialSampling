"""
    Interpolation of the CFD data for the specified fields and time steps onto the coarse grid, sampled with the S^3
    algorithm. Export the interpolated data to HDF5 and XDMF to be able to load them into Paraview.
"""
import logging
import torch as pt

from time import time
from sklearn.neighbors import KNeighborsRegressor

from .data import Datawriter
from .sparse_spatial_sampling import SparseSpatialSampling
from .const import GRID, CONST, FACES, CENTERS, VERTICES, DATA

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

pt.set_default_dtype(pt.float64)


class Fields:
    """
    class for storing each interpolated field at cell centers and cell nodes.
    """
    def __init__(self, centers: pt.Tensor = None, vertices: pt.Tensor = None):
        self.centers = centers
        self.vertices = vertices


class ExportData:
    def __init__(self, s_cube: SparseSpatialSampling, write_new_file_for_each_field: bool = False, n_jobs: int = None):
        """
        Implements an Export class for interpolating the original snapshots onto the grid generated by S^3 and export
        them to HDF5

        :param s_cube: s_cube object containing the sampled grid
        :param write_new_file_for_each_field: flag if each field should be written to a new HDF5 file (False) or if all
                                              fields should be written to a single file (True)
        :param n_jobs: number of CPUs used for interpolation.
                       If 'None' then the same number of CPUs used for executing S^3 will be used
        """
        self._new_file = write_new_file_for_each_field

        # properties we get from the s_cube object
        self.n_dimensions = s_cube.n_dimensions
        self._face_id = s_cube.faces
        self._centers = s_cube.centers
        self._vertices = s_cube.vertices
        self._levels = s_cube.levels
        self._metric = s_cube.metric
        self._size_initial_cell = s_cube.size_initial_cell
        self._save_dir = s_cube.save_path
        self._save_name = s_cube.save_name
        self._grid_name = s_cube.grid_name
        self._write_times = s_cube.write_times

        # properties we need for interpolating and exporting the fields
        self._interpolated_fields = Fields()
        self._field_name = None
        self._datawriter = None
        self._knn = KNeighborsRegressor(n_neighbors=8 if self.n_dimensions == 2 else 26, weights="distance",
                                        n_jobs=s_cube.n_jobs if n_jobs is None else n_jobs)
        self._snapshot_counter = 0
        self._field_name = None
        self._initialized = False
        self._finished = False
        self._n_snapshots_total = None
        self._t_start = time()

    def export(self, _coord: pt.Tensor, _data: pt.Tensor, _field_name: str,
               _n_snapshots_total: int = None) -> None:
        """
        interpolate the provided (original) CFD data onto the generated grid by S^3 and write it to a HDF5 and XDMF
        file for all given time steps.

        :param _coord: the coordinates of the original grid used in CFD
        :param _data: the original field data with dimension [N_cells, N_dimensions, N_snapshots].
                      N_snapshots can either be all snapshots, a batch of snapshots or just a single snapshot
        :param _field_name: name of the field, which should be exported, e.g. 'p' for pressure field
        :param _n_snapshots_total: number of snapshots in total, which should be exported. If 'None', it is assumed
                                   that the provided data are all available snapshots
        :return: None
        """
        # make sure the field name is always up to date
        self._field_name = _field_name
        self._fit_data(_coord, _data, _field_name, _n_snapshots_total)
        self._write_data_to_hdmf5()

    def _fit_data(self, _coord: pt.Tensor, _data: pt.Tensor, _field_name: str, _n_snapshots_total: int = None) -> None:
        """
        Interpolate the CFD data executed on the original grid onto the newly, coarser grid generated by the S^3
        algorithm. The original field is interpolated at the cell centers as well as at the cell nodes.

        Note: the variables 'centers' & 'vertices' used in this method are denoting the values of the field at center
        and nodes of each cell (not the node coordinates of the generated mesh)

        :param _coord: the coordinates of the original grid used in CFD
        :param _data: the original field data
        :param _field_name: name of the field, which should be exported, e.g. 'p' for pressure field
        :param _n_snapshots_total: number of snapshots in total, which should be exported. If 'None', it is assumed
                                   that the provided data are all available snapshots
        :return: None
        """
        # check if the field has the correct shape -> scalar fields need to be unsqueezed at dim=1 as:
        # [N_cells, N_dimensions, N_snapshots] (vector field) or [N_cells, 1, N_snapshots] (scalar field)
        assert len(_data.size()) == 3, "The provided field must have the shape '[N_cells, N_dimensions, N_snapshots]'" \
                                       "for a vector field and '[N_cells, 1, N_snapshots]' for a scalar field"
        if not self._initialized:
            logger.info(f"Starting interpolation and export of field {self._field_name}.")

        # fit the metric at some point. If the number of coordinates is still the same as the CFD grid, then we haven't
        # fitted it yet.
        if self._metric.size(0) == _coord.size(0):
            self._knn.fit(_coord, self._metric)

            # overwrite the metric with the interpolated one
            self._metric = pt.from_numpy(self._knn.predict(self._centers))

        # determine the required size of the data matrix
        self._n_snapshots_total = _n_snapshots_total if _n_snapshots_total is not None else _data.size()[-1]

        # create empty tensors for the field values at centers & vertices with dimensions
        # [N_cells, N_dimensions, N_snapshots_currently] each call to allow variable batch sizes
        self._interpolated_fields.centers = pt.zeros((self._centers.size()[0], _data.size()[1], _data.size()[2]),
                                                     dtype=pt.float32)
        self._interpolated_fields.vertices = pt.zeros((self._vertices.size()[0], _data.size()[1], _data.size()[2]),
                                                      dtype=pt.float32)

        # fit the KNN and interpolate the data, we need to predict each dimension separately (otherwise dim. mismatch)
        for dimension in range(_data.size()[1]):
            self._knn.fit(_coord, _data[:, dimension, :])
            self._interpolated_fields.vertices[:, dimension, :] = pt.from_numpy(self._knn.predict(self._vertices))
            self._interpolated_fields.centers[:, dimension, :] = pt.from_numpy(self._knn.predict(self._centers))

        # update the number of snapshots we already interpolated
        self._snapshot_counter += _data.size()[-1]

    def _write_data_to_hdmf5(self) -> None:
        """
        Write the generated grid and the fields, interpolated at the cell centers and nodes, respectively, to an HDF5
        file for the given number of snapshots.

        :return: None
        """
        # create a writer and datasets for the grid if on initial call
        if not self._initialized:
            logger.info(f"Writing HDF5 file for field {self._field_name}.")

            # create datawriter instance
            if self._new_file:
                self._datawriter = Datawriter(self._save_dir, f"{self._save_name}_{self._field_name}.h5")
            else:
                self._datawriter = Datawriter(self._save_dir, f"{self._save_name}.h5")

            # write the grid
            self._datawriter.write_data(FACES, group=GRID, data=self._face_id)
            self._datawriter.write_data(VERTICES, group=GRID, data=self._vertices)
            self._datawriter.write_data(CENTERS, group=GRID, data=self._centers)

            # add field for the cell levels, metric and initial cell size (used for computing cell areas or volumes)
            self._datawriter.write_data("levels", group=CONST, data=self._levels)
            self._datawriter.write_data("metric", group=CONST, data=self._metric)
            self._datawriter.write_data("size_initial_cell", group=CONST, data=self._size_initial_cell)
            self._initialized = True

        else:
            # once the grid is written, we can append the field data
            self._datawriter.mode = "a"

        # write the datasets for each given time step, we already updated the snapshot counter after fitting the
        # field, so we need to subtract it to get the starting dt
        t_start = self._snapshot_counter - self._interpolated_fields.centers.size(-1)
        t_end = self._snapshot_counter

        # create a group for each specified field
        for i, t in enumerate(self._write_times[t_start:t_end]):
            # in case we have a scalar, we need to remove the additional dimension we created for fitting the data
            if self._interpolated_fields.centers.size(1) == 1:
                self._datawriter.write_data(f"{self._field_name}_center", group=DATA, time_step=str(t),
                                            data=self._interpolated_fields.centers.squeeze(1)[:, i])
                self._datawriter.write_data(f"{self._field_name}_vertices", group=DATA, time_step=str(t),
                                            data=self._interpolated_fields.vertices.squeeze(1)[:, i])
            # in case we have a vector
            else:
                self._datawriter.write_data(f"{self._field_name}_center", group=DATA, time_step=str(t),
                                            data=self._interpolated_fields.centers[:, :, i])
                self._datawriter.write_data(f"{self._field_name}_vertices", group=DATA, time_step=str(t),
                                            data=self._interpolated_fields.vertices[:, :, i])

        # check if we have written all snapshots, if yes, then write the XDMF file
        if self._snapshot_counter == self._n_snapshots_total:
            # close hdf file after writing
            self._datawriter.close()

            # the XDMF writer updates XDMF every time we are finished writing a field, so each new gets added
            # automatically if we specified to write everything in the same file
            self._datawriter.write_xdmf_file()

            # reset properties for the next field, we don't need to reset the computed cell areas since the grid remains
            # the same for all fields
            self._interpolated_fields = Fields()
            self._snapshot_counter = 0

            if self._new_file:
                self._initialized = False

            logger.info(f"Finished export of field {self._field_name} in {round(time() - self._t_start, 3)}s.")
            self._t_start = time()

    @property
    def write_times(self) -> list:
        return self._write_times

    @write_times.setter
    def write_times(self, value: list) -> None:
        self._write_times = value

    @property
    def new_file(self):
        return self._new_file


if __name__ == "__main__":
    pass
